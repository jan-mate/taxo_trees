{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_culinary_fruits' \n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_birds_of_Denmark' \n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_mammals_of_Denmark'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_vegetables'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_leaf_vegetables'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_culinary_nuts'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/Plants_used_as_herbs_or_spices'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_edible_seeds'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_edible_flowers'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_forageable_plants'\n",
    "urls_to_scrape = 'https://en.wikipedia.org/wiki/Root_vegetable'\n",
    "\n",
    "name = urls_to_scrape.split('/')[-1]\n",
    "    \n",
    "ranking_metric_choice = None  # Options: 'backlink', 'page_views', or None\n",
    "top_n_articles = 10000\n",
    "\n",
    "start_index = 0\n",
    "end_index = 10000\n",
    "tiny = (end_index - start_index) < 50\n",
    "output_filename = f\"{name}_tiny.json\" if tiny else f\"{name}.json\"\n",
    "\n",
    "article_type = 10\n",
    "\n",
    "taxon_synonyms = {\n",
    "    'tracheophytes': 'Tracheophyta',\n",
    "    'tracheophyta': 'Tracheophyta',\n",
    "    'seed plant': 'Spermatophyta',\n",
    "    'seed plants': 'Spermatophyta',\n",
    "    'angiosperms': 'Magnoliophyta',\n",
    "    'angiosperm': 'Magnoliophyta',\n",
    "    'magnoliophyta': 'Magnoliophyta',\n",
    "    'plantae': 'Plantae',\n",
    "    'plant': 'Plantae',\n",
    "    'eudicots': 'Eudicotyledons',\n",
    "    'eudicot': 'Eudicotyledons',\n",
    "    'rosids': 'Rosids',\n",
    "    'rosid': 'Rosids',\n",
    "    'monocots': 'Liliopsida',\n",
    "    'monocot': 'Liliopsida',\n",
    "    'liliopsida': 'Liliopsida',\n",
    "    'magnoliids': 'Magnoliidae',\n",
    "    'magnoliid': 'Magnoliidae',\n",
    "    'spermatophytes': 'Spermatophyta',\n",
    "    'eukaryota': 'Eukaryota',\n",
    "    'archaeplastida': 'Archaeplastida',\n",
    "    'viridiplantae': 'Viridiplantae',\n",
    "    'streptophyta': 'Streptophyta',\n",
    "    'diaphoretickes': 'Diaphoretickes',\n",
    "    'euphyllophyta': 'Euphyllophyta',\n",
    "    'ericales': 'Ericales',\n",
    "    'animal':'Animalia'\n",
    "}\n",
    "\n",
    "standard_ranks = [\n",
    "    'superkingdom', 'kingdom', 'subkingdom', 'infrakingdom',\n",
    "    'superphylum', 'phylum', 'subphylum', \n",
    "    'superclass', 'class', 'subclass', 'infraclass',\n",
    "    'superorder', 'order', 'suborder', 'infraorder',\n",
    "    'superfamily', 'family', 'subfamily', 'tribe', 'subtribe',\n",
    "    'genus', 'subgenus', 'species', 'subspecies',\n",
    "]\n",
    "\n",
    "# Wikipedia doesn't have good data on other ranks :( These ones work for plants\n",
    "standard_ranks = ['superkingdom', 'kingdom', 'subkingdom', 'infrakingdom','superdivision', 'division', 'subdivision', 'order', 'family', 'genus', 'species']#\n",
    "# These work for animals, because apperently division/phylum are the same taxonomic rank but which to use depends on which kingdom???\n",
    "# If I knew this I would've fixed it earlier on, but it's not worth it anymore\n",
    "# standard_ranks = ['superkingdom', 'kingdom', 'subkingdom', 'infrakingdom','superphylum', 'phylum', 'subphylum', 'superclass', 'class', 'subclass', 'infraclass', 'order', 'family', 'genus', 'species']\n",
    "\n",
    "def scrape_initial_wiki_page(url, start=30, end=40, article_type=article_type):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    if article_type in [7, 8]: # The articles in this list depend on time of request\n",
    "        content = soup.find('div', {'id': 'bodyContent'})\n",
    "    else:\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "\n",
    "    title_data = []\n",
    "\n",
    "    if article_type == 0:  # Fruits\n",
    "        tables = content.find_all('table', {'class': 'wikitable'})\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    a_tag = cells[0].find('a', href=True)\n",
    "                    i_tag = cells[1].find('i')\n",
    "                    if a_tag and i_tag:\n",
    "                        text = a_tag.text.strip()\n",
    "                        href = a_tag.get('href')\n",
    "                        species_name = i_tag.text.strip()\n",
    "                        if text and href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                            title_data.append({\n",
    "                                'title': text,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'species': species_name\n",
    "                            })\n",
    "\n",
    "    elif article_type == 1: # Danish birds\n",
    "        list_items = content.find_all('li')\n",
    "        for item in list_items:\n",
    "            i_tags = item.find_all('i')\n",
    "            if i_tags:\n",
    "                species_name = i_tags[-1].text.strip()\n",
    "                title_text = item.get_text(separator=' ', strip=True)\n",
    "                \n",
    "                common_name_text = re.sub(rf'\\b{species_name}\\b', '', title_text).strip()\n",
    "                common_name_text = re.sub(r'\\(\\s*', '(', common_name_text)\n",
    "                common_name_text = re.sub(r'\\s*\\)', ')', common_name_text)\n",
    "\n",
    "                a_tag = item.find('a', href=True)\n",
    "                if a_tag:\n",
    "                    href = a_tag.get('href')\n",
    "                    if href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                        title_data.append({\n",
    "                            'title': common_name_text,\n",
    "                            'link': f\"https://en.wikipedia.org{href}\",\n",
    "                            'species': species_name\n",
    "                        })\n",
    "\n",
    "    elif article_type == 2:  # Danish Mammals\n",
    "        list_items = content.find_all('li')\n",
    "        for item in list_items:\n",
    "            a_tag = item.find('a', href=True)\n",
    "            i_tag = item.find('i')\n",
    "            \n",
    "            if a_tag and i_tag:\n",
    "                common_name_with_status = a_tag.text.strip()\n",
    "                species_name = i_tag.text.strip()\n",
    "\n",
    "                href = a_tag.get('href')\n",
    "                if href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                    title_data.append({\n",
    "                        'title': common_name_with_status,\n",
    "                        'link': f\"https://en.wikipedia.org{href}\",\n",
    "                        'species': species_name\n",
    "                    })\n",
    "    \n",
    "    elif article_type == 3:  # Herbs or Spices\n",
    "        table = content.find('table', {'class': 'wikitable'})\n",
    "        if table:\n",
    "            rows = table.find_all('tr')[1:] \n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    title = cells[0].get_text(strip=True)\n",
    "                    species = cells[1].get_text(strip=True)\n",
    "                    a_tag = cells[0].find('a', href=True)\n",
    "                    href = a_tag.get('href') if a_tag else ''\n",
    "                    if species and href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                        title_data.append({\n",
    "                            'title': title,\n",
    "                            'link': f\"https://en.wikipedia.org{href}\",\n",
    "                            'species': species\n",
    "                        })\n",
    "                        \n",
    "    elif article_type == 4: # (pseudo)cereals\n",
    "        tables = content.find_all('table', {'class': 'wikitable'})\n",
    "        cereals_table, pseudocereals_table = None, None\n",
    "\n",
    "        for table in tables:\n",
    "            title_row = table.find('tr')\n",
    "            if title_row:\n",
    "                th = title_row.find('th')\n",
    "                if th and \"Cereals\" in th.get_text(strip=True):\n",
    "                    cereals_table = table\n",
    "                if th and \"Pseudocereals\" in th.get_text(strip=True):\n",
    "                    pseudocereals_table = table\n",
    "                if cereals_table and pseudocereals_table:\n",
    "                    break\n",
    "\n",
    "\n",
    "        def extract_table_data(table, has_tribe, is_pseudocereal=False):\n",
    "            entries = []\n",
    "            rows = table.find_all('tr')\n",
    "            current_family = None\n",
    "            current_tribe = None\n",
    "            current_genus = None\n",
    "            family_rowspan = 0\n",
    "            tribe_rowspan = 0\n",
    "            genus_rowspan = 0\n",
    "\n",
    "            for index, row in enumerate(rows):\n",
    "                if index < 2:\n",
    "                    continue\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                cell_ptr = 0\n",
    "                family = current_family\n",
    "                tribe = current_tribe\n",
    "                genus = current_genus\n",
    "            \n",
    "                \n",
    "                if family_rowspan > 0:\n",
    "                    family_rowspan -= 1\n",
    "                else:\n",
    "                    if cell_ptr < len(cells):\n",
    "                        family_cell = cells[cell_ptr]\n",
    "                        family = family_cell.get_text(strip=True)\n",
    "                        if 'rowspan' in family_cell.attrs:\n",
    "                            family_rowspan = int(family_cell['rowspan']) - 1\n",
    "                        current_family = family\n",
    "                        cell_ptr += 1\n",
    "                \n",
    "                if has_tribe:\n",
    "                    if tribe_rowspan > 0:\n",
    "                        tribe_rowspan -= 1\n",
    "                    else:\n",
    "                        if cell_ptr < len(cells):\n",
    "                            tribe_cell = cells[cell_ptr]\n",
    "                            tribe = tribe_cell.get_text(strip=True)\n",
    "                            if 'rowspan' in tribe_cell.attrs:\n",
    "                                tribe_rowspan = int(tribe_cell['rowspan']) - 1\n",
    "                            current_tribe = tribe\n",
    "                            cell_ptr += 1\n",
    "                \n",
    "                if genus_rowspan > 0:\n",
    "                    genus_rowspan -= 1\n",
    "                else:\n",
    "                    if cell_ptr < len(cells):\n",
    "                        genus_cell = cells[cell_ptr]\n",
    "                        genus = genus_cell.get_text(strip=True)\n",
    "                        if 'rowspan' in genus_cell.attrs:\n",
    "                            genus_rowspan = int(genus_cell['rowspan']) - 1\n",
    "                        current_genus = genus\n",
    "                        cell_ptr += 1\n",
    "                \n",
    "                if cell_ptr < len(cells):\n",
    "                    species_cell = cells[cell_ptr]\n",
    "                    species_link = species_cell.find('a', href=True)\n",
    "                    if species_link:\n",
    "                        species = species_link.get_text(strip=True)\n",
    "                        species_href = species_link['href']\n",
    "                    else:\n",
    "                        species = species_cell.get_text(strip=True)\n",
    "                        species_href = ''\n",
    "                    cell_ptr += 1\n",
    "                else:\n",
    "                    species = ''\n",
    "                    species_href = ''\n",
    "                \n",
    "                if cell_ptr < len(cells):\n",
    "                    seed_names_cell = cells[cell_ptr]\n",
    "                    seed_names = seed_names_cell.get_text(strip=True)\n",
    "                    cell_ptr += 1\n",
    "                else:\n",
    "                    seed_names = ''\n",
    "\n",
    "                \n",
    "                if species and seed_names and species_href:\n",
    "                    if is_pseudocereal:\n",
    "                        seed_names += '*'\n",
    "                    entry = {\n",
    "                        'title': seed_names,\n",
    "                        'species': species,\n",
    "                        'link': f\"https://en.wikipedia.org{species_href}\"\n",
    "                    }\n",
    "                    # print(\"added entry:\", entry)\n",
    "                    entries.append(entry)\n",
    "            # print(f\"Total entries extracted from table: {len(entries)}\")\n",
    "            return entries\n",
    "\n",
    "\n",
    "\n",
    "        if cereals_table:\n",
    "            table_data = extract_table_data(cereals_table, has_tribe=True, is_pseudocereal=False)\n",
    "            title_data.extend(table_data)\n",
    "        if pseudocereals_table:\n",
    "            table_data = extract_table_data(pseudocereals_table, has_tribe=False, is_pseudocereal=True)\n",
    "            title_data.extend(table_data)\n",
    "\n",
    "\n",
    "        def extract_specific_ul_data(content):\n",
    "            entries = []\n",
    "            specific_p = content.find('p', string=lambda text: text and \"Other grasses with edible seeds include:\" in text)\n",
    "            if specific_p:\n",
    "                specific_ul = specific_p.find_next_sibling('ul')\n",
    "                if specific_ul:\n",
    "                    li_elements = specific_ul.find_all('li')\n",
    "                    for index, li in enumerate(li_elements):\n",
    "                        try:\n",
    "                            species_link = li.find('a', href=True)\n",
    "                            species_href = species_link['href'] if species_link else ''\n",
    "                            species = species_link.get_text(strip=True) if species_link else ''\n",
    "                            parts = li.get_text(separator=\" \", strip=True).split('–')\n",
    "                            seed_name = parts[-1].strip() if len(parts) >= 2 else parts[0].strip()\n",
    "                            print(f\"List item {index +1}: Species='{species}', Seed names='{seed_name}', Link='{species_href}'\")\n",
    "                            if seed_name and species and species_href and len(seed_name) < 25:\n",
    "                                entry = {\n",
    "                                    'title': seed_name,\n",
    "                                    'species': species,\n",
    "                                    'link': f\"https://en.wikipedia.org{species_href}\"\n",
    "                                }\n",
    "                                entries.append(entry)\n",
    "                            else:\n",
    "                                print(f\"Skipping list item {index +1}: Incomplete data.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing list item {index +1}: {e}\")\n",
    "                            continue\n",
    "            print(f\"Total entries extracted from list: {len(entries)}\")\n",
    "            return entries\n",
    "\n",
    "        specific_ul_data = extract_specific_ul_data(content)\n",
    "        title_data.extend(specific_ul_data)\n",
    "\n",
    "        manual_entries = [\n",
    "            {\n",
    "                'title': 'Wild rice',\n",
    "                'species': 'Zizania palustris',\n",
    "                'link': 'https://en.wikipedia.org/wiki/Wild_rice'\n",
    "            },\n",
    "            {\n",
    "                'title': 'love-lies-bleeding*',\n",
    "                'species': 'Amaranthus caudatus',\n",
    "                'link': 'https://en.wikipedia.org/wiki/Amaranthus_caudatus'\n",
    "            },\n",
    "            {\n",
    "                'title': 'prince-of-Wales feather*',\n",
    "                'species': 'Amaranthus hypochondriacus',\n",
    "                'link': 'https://en.wikipedia.org/wiki/Amaranthus_hypochondriacus'\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        title_data.extend(manual_entries)\n",
    "\n",
    "    elif article_type == 5:  # Veggies\n",
    "        tables = content.find_all('table')\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]: \n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    a_tag = cells[0].find('a', href=True)\n",
    "                    species_cell = cells[1]\n",
    "                    \n",
    "                    species_name = species_cell.get_text(separator=\" \", strip=True)\n",
    "                    \n",
    "                    if a_tag:\n",
    "                        text = a_tag.text.strip()\n",
    "                        href = a_tag.get('href')\n",
    "                        if (\n",
    "                            text \n",
    "                            and href.startswith('/wiki/') \n",
    "                            and not href.startswith('/wiki/Special:') \n",
    "                            and ':' not in href\n",
    "                        ):\n",
    "                            title_data.append({\n",
    "                                'title': text,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'species': species_name\n",
    "                            })\n",
    "\n",
    "    elif article_type == 6: # Leafy veggies\n",
    "        tables = content.find_all('table', {'class': 'wikitable'})\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    a_tag = cells[0].find('a', href=True)\n",
    "                    i_tag = cells[0].find('i')\n",
    "                    if a_tag and i_tag:\n",
    "                        species_name = a_tag.text.strip()\n",
    "                        href = a_tag.get('href')\n",
    "                        common_name = cells[1].text.strip()\n",
    "                        if species_name and href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                            title_data.append({\n",
    "                                'species': species_name,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'title': common_name\n",
    "                            })\n",
    "    \n",
    "    elif article_type == 7: # nuts\n",
    "        list_items = content.find_all('li')\n",
    "        for item in list_items:\n",
    "            a_tag = item.find('a', href=True)\n",
    "            if a_tag:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                    common_name_text = a_tag.get_text(strip=True)\n",
    "                    species_match = re.search(r'\\((.*?)\\)', item.text)\n",
    "                    if species_match:\n",
    "                        species_name = species_match.group(1).strip()\n",
    "                        if \"spp\" not in species_name.lower():\n",
    "                            title_data.append({\n",
    "                                'title': common_name_text,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'species': species_name\n",
    "                            })\n",
    "\n",
    "    elif article_type == 8:  # Flowers\n",
    "        tables = content.find_all('table', {'class': 'wikitable'})\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]:  # Skip header row\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 4:  # Ensure there are at least 4 columns\n",
    "                    a_tag = cells[0].find('a', href=True)\n",
    "                    i_tag = cells[0].find('i')\n",
    "                    common_name = cells[3].text.strip()\n",
    "                    if a_tag and i_tag:\n",
    "                        species_name = a_tag.text.strip()\n",
    "                        href = a_tag.get('href')\n",
    "                        if species_name and href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                            title_data.append({\n",
    "                                'species': species_name,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'title': common_name\n",
    "                            })\n",
    "\n",
    "    elif article_type == 9: # Foregable\n",
    "        tables = content.find_all('table')\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows[1:]: \n",
    "                print(f\"Processing table with {len(rows) - 1} rows (excluding header).\")\n",
    "\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 4:\n",
    "                    common_name = cells[2].text.strip()\n",
    "                    a_tag = cells[3].find('a', href=True)\n",
    "                    i_tag = cells[3].find('i')\n",
    "                    if a_tag and i_tag:\n",
    "                        species_name = a_tag.text.strip()\n",
    "                        href = a_tag.get('href')\n",
    "\n",
    "                        if species_name and href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href:\n",
    "                            title_data.append({\n",
    "                                'species': species_name,\n",
    "                                'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                'title': common_name\n",
    "                            })\n",
    "\n",
    "    elif article_type == 10: # roots\n",
    "        list_items = content.find_all('li')\n",
    "        for item in list_items:\n",
    "            nested_ul = item.find('ul')\n",
    "            if nested_ul:\n",
    "                nested_lis = nested_ul.find_all('li')\n",
    "                for nested_item in nested_lis:\n",
    "                    a_tag = nested_item.find('a', href=True)\n",
    "                    i_tag = nested_item.find('i')\n",
    "                    if a_tag and i_tag:\n",
    "                        species_name = a_tag.text.strip()\n",
    "                        href = a_tag['href']\n",
    "                        if href.startswith('/wiki/') and not href.startswith('/wiki/Special:') and ':' not in href and \"spp\" not in species_name.lower():\n",
    "                            common_name_match = re.search(r'\\((.*?)\\)', nested_item.text)\n",
    "                            if common_name_match:\n",
    "                                common_name = common_name_match.group(1).strip()\n",
    "                                title_data.append({\n",
    "                                    'title': common_name,\n",
    "                                    'link': f\"https://en.wikipedia.org{href}\",\n",
    "                                    'species': species_name\n",
    "                                })\n",
    "\n",
    "    print(title_data[start:end])\n",
    "    return title_data[start:end]\n",
    "\n",
    "# Only some articles have QID, and I get different kind of data through direct scraping and QID. The data will be combined using propagate_taxonomic_info()\n",
    "def get_qid_from_title(article_title, lang=\"en\"):\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": article_title,\n",
    "        \"prop\": \"pageprops\",\n",
    "        \"format\": \"json\",\n",
    "        \"ppprop\": \"wikibase_item\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if 'query' in data and 'pages' in data['query']:\n",
    "        pages = data['query']['pages']\n",
    "        for page_id, page in pages.items():\n",
    "            if \"pageprops\" in page and \"wikibase_item\" in page[\"pageprops\"]:\n",
    "                return page[\"pageprops\"][\"wikibase_item\"]\n",
    "    return None\n",
    "\n",
    "def search_wikidata(query):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": query,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "        \"limit\": 1,\n",
    "        \"type\": \"item\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if data['search']:\n",
    "        return data['search'][0]['id']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_taxonomy_data(qid):\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    query = \"\"\"\n",
    "    SELECT ?parent ?parentLabel ?taxonRank ?taxonRankLabel\n",
    "    WHERE {\n",
    "      wd:%s wdt:P171* ?parent .\n",
    "      OPTIONAL { ?parent wdt:P105 ?taxonRank . }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "    }\n",
    "    \"\"\" % qid\n",
    "    headers = {\n",
    "        'Accept': 'application/sparql-results+json',\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params={'query': query})\n",
    "    data = response.json()\n",
    "    taxonomy = {}\n",
    "    for entry in data['results']['bindings']:\n",
    "        rank = entry.get('taxonRankLabel', {}).get('value', '').lower()\n",
    "        taxon = entry['parentLabel']['value']\n",
    "        if rank and rank in standard_ranks:\n",
    "            taxon_standard = taxon_synonyms.get(taxon.lower(), taxon)\n",
    "            taxonomy[rank] = taxon_standard\n",
    "    return taxonomy\n",
    "\n",
    "def get_taxonomic_data_from_wikipedia(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    taxobox = soup.find('table', {'class': 'infobox biota'})\n",
    "    if taxobox:\n",
    "        taxonomic_data = {}\n",
    "        rows = taxobox.find_all('tr')\n",
    "        clade_count = 1\n",
    "        for row in rows:\n",
    "            tds = row.find_all('td')\n",
    "            if len(tds) == 2:\n",
    "                rank = tds[0].get_text(strip=True).replace(\":\", \"\").lower()\n",
    "                value = tds[1].get_text(strip=True)\n",
    "                rank = rank.strip().lower()\n",
    "                if \"clade\" in rank:\n",
    "                    taxonomic_data[f\"clade_{clade_count}\"] = value\n",
    "                    clade_count += 1\n",
    "                elif rank:\n",
    "                    value_standard = taxon_synonyms.get(value.lower(), value)\n",
    "                    taxonomic_data[rank] = value_standard\n",
    "        taxonomic_data['title'] = soup.find('h1', {'id': 'firstHeading'}).get_text(strip=True)\n",
    "        taxonomic_data['link'] = url\n",
    "        taxonomic_data['QID'] = None\n",
    "        return taxonomic_data\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def get_image_and_text_from_wikipedia(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    taxobox = soup.find('table', {'class': 'infobox biota'})\n",
    "    img_link = None\n",
    "    if taxobox:\n",
    "        img = taxobox.find('img')\n",
    "        if img:\n",
    "            src = img.get('src')\n",
    "            if src:\n",
    "                if src.startswith('//'):\n",
    "                    img_link = 'https:' + src\n",
    "                else:\n",
    "                    img_link = src\n",
    "    if not img_link:\n",
    "        img = soup.find('img')\n",
    "        if img:\n",
    "            src = img.get('src')\n",
    "            if src:\n",
    "                if src.startswith('//'):\n",
    "                    img_link = 'https:' + src\n",
    "                else:\n",
    "                    img_link = src\n",
    "    text = None\n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    if content:\n",
    "        paragraphs = content.find_all('p', recursive=False)\n",
    "        for paragraph in paragraphs:\n",
    "            for sup in paragraph.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            text_content = paragraph.get_text(separator=' ', strip=True)\n",
    "            if text_content:\n",
    "                text_content = re.sub(r'\\s+([.,!?;:])', r'\\1', text_content)\n",
    "                text_content = re.sub(r'\\s{2,}', ' ', text_content)\n",
    "                text = text_content\n",
    "                break\n",
    "\n",
    "    return img_link, text if text else \"No description available\"\n",
    "\n",
    "def get_pageviews_bulk(titles):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    pageviews = {}\n",
    "    for i in range(0, len(titles), 50):\n",
    "        batch = titles[i:i+50]\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'titles': '|'.join(batch),\n",
    "            'prop': 'pageviews',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        if 'query' in data and 'pages' in data['query']:\n",
    "            pages = data['query']['pages']\n",
    "            for page_id in pages:\n",
    "                page = pages[page_id]\n",
    "                title = page['title']\n",
    "                views = page.get('pageviews', {})\n",
    "                total_views = sum(v for v in views.values() if v)\n",
    "                pageviews[title] = total_views\n",
    "    return pageviews\n",
    "\n",
    "def get_backlinks_count(title):\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'backlinks',\n",
    "        'bltitle': title,\n",
    "        'bllimit': 'max',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    total_backlinks = 0\n",
    "    while True:\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        backlinks = data.get('query', {}).get('backlinks', [])\n",
    "        total_backlinks += len(backlinks)\n",
    "        if 'continue' in data:\n",
    "            params.update(data['continue'])\n",
    "        else:\n",
    "            break\n",
    "    return total_backlinks\n",
    "\n",
    "def get_backlinks_counts(titles):\n",
    "    backlinks_counts = {}\n",
    "    for i, title in enumerate(titles):\n",
    "        count = get_backlinks_count(title)\n",
    "        backlinks_counts[title] = count\n",
    "    return backlinks_counts\n",
    "\n",
    "# Fill in blanks, fx if two species has same genus, I assume they have same family and so on (this isn't 100% correct, but it gives me way more data)\n",
    "def propagate_taxonomic_info(df, taxonomic_ranks):\n",
    "    for current_rank, parent_rank in zip(taxonomic_ranks[::-1], taxonomic_ranks[-2::-1]):\n",
    "        unique_items = df[current_rank].dropna().unique()\n",
    "        \n",
    "        for item in unique_items:\n",
    "            most_common_parent = (\n",
    "                df[df[current_rank] == item][parent_rank]\n",
    "                .dropna()\n",
    "                .mode()\n",
    "            )\n",
    "            most_common_parent = most_common_parent.iloc[0] if not most_common_parent.empty else None\n",
    "            print(f\"Most common {parent_rank} for {current_rank} {item}: {most_common_parent}\")\n",
    "            \n",
    "            df.loc[df[current_rank] == item, parent_rank] = most_common_parent\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_species(name):\n",
    "    parts = re.split(r'[.\\s]+', name)\n",
    "    if len(parts) > 1:\n",
    "        return parts[1]\n",
    "    return name\n",
    "    \n",
    "def filter_img_links(link):\n",
    "    if '.svg' in link:\n",
    "        return ''\n",
    "    else:\n",
    "        return link\n",
    "\n",
    "def process_links(urls, ranking_metric=None, start=0, end=10, top_n=100, output_file=\"taxonomy_data.json\"):\n",
    "    if isinstance(urls, str):\n",
    "        urls = [urls]\n",
    "    all_title_data = []\n",
    "    for url in urls:\n",
    "        title_data = scrape_initial_wiki_page(url, start=start, end=end)\n",
    "        all_title_data.extend(title_data)\n",
    "    all_title_data = [dict(t) for t in {tuple(d.items()) for d in all_title_data}]\n",
    "    titles = [item['title'] for item in all_title_data]\n",
    "    if ranking_metric == 'page_views':\n",
    "        ranking_dict = get_pageviews_bulk(titles)\n",
    "        sort_key = 'pageviews'\n",
    "    elif ranking_metric == 'backlink':\n",
    "        ranking_dict = get_backlinks_counts(titles)\n",
    "        sort_key = 'backlinks'\n",
    "    else:\n",
    "        sort_key = None\n",
    "    if sort_key:\n",
    "        for item in all_title_data:\n",
    "            item[sort_key] = ranking_dict.get(item['title'], 0)\n",
    "        all_title_data = sorted(all_title_data, key=lambda x: x.get(sort_key, 0), reverse=True)\n",
    "    if isinstance(top_n, float):\n",
    "        num_articles = int(top_n * len(all_title_data))\n",
    "    else:\n",
    "        num_articles = int(top_n)\n",
    "    all_title_data = all_title_data[:num_articles]\n",
    "    results = []\n",
    "    for item in all_title_data:\n",
    "        title = item['title']\n",
    "        link = item['link']\n",
    "        species = item.get('species', None)\n",
    "        qid = None\n",
    "        if species:\n",
    "            qid = get_qid_from_title(species)\n",
    "            if not qid:\n",
    "                qid = search_wikidata(species)\n",
    "        else:\n",
    "            qid = get_qid_from_title(title)\n",
    "            if not qid:\n",
    "                qid = search_wikidata(title)\n",
    "        taxonomy = None\n",
    "        if qid:\n",
    "            taxonomy = get_taxonomy_data(qid)\n",
    "        result_dict = {\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'QID': qid if qid else None\n",
    "        }\n",
    "        if species:\n",
    "            result_dict['species'] = species\n",
    "        if sort_key:\n",
    "            result_dict[sort_key] = item.get(sort_key, 0)\n",
    "        if taxonomy:\n",
    "            for rank, taxon in taxonomy.items():\n",
    "                if rank not in result_dict:\n",
    "                    result_dict[rank] = taxon\n",
    "        else:\n",
    "            taxonomy = get_taxonomic_data_from_wikipedia(link)\n",
    "            if taxonomy:\n",
    "                for key, value in taxonomy.items():\n",
    "                    if key not in result_dict:\n",
    "                        result_dict[key] = value\n",
    "        img_link, text = get_image_and_text_from_wikipedia(link)\n",
    "        result_dict['img_link'] = filter_img_links(img_link) if img_link else ''\n",
    "        result_dict['text'] = text if text else \"No description available\"\n",
    "        results.append(result_dict)\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    columns_order = ['title', 'link', 'QID', 'img_link', 'text']\n",
    "    if sort_key:\n",
    "        columns_order.append(sort_key)\n",
    "    columns_order += standard_ranks\n",
    "    columns_order = [col for col in columns_order if col in df.columns]\n",
    "    df = df[columns_order]\n",
    "    #taxonomic_ranks = ['superkingdom', 'kingdom', 'subkingdom', 'infrakingdom','superdivision', 'division', 'subdivision', 'order', 'family', 'genus']\n",
    "    #taxonomic_ranks = ['superkingdom', 'kingdom', 'subkingdom', 'infrakingdom','superphylum', 'phylum', 'subphylum', 'order', 'family', 'genus']\n",
    "    df = propagate_taxonomic_info(df, standard_ranks)\n",
    "    df = df.dropna(subset=['genus', 'species'])\n",
    "    df['species_cleaned'] = df['species'].apply(extract_species)\n",
    "    df.drop_duplicates(subset=['species_cleaned', 'title'], inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    df['species_info'] = df.apply(lambda row: {\n",
    "        'species_name': row['species_cleaned'],\n",
    "        'title': row['title'],\n",
    "        'link': row['link'],\n",
    "        'img_link': row['img_link'],\n",
    "        'text': row['text']\n",
    "    }, axis=1)\n",
    "    df.to_json(output_file, orient='records', lines=True)\n",
    "    return df\n",
    "\n",
    "final_df = process_links(\n",
    "    urls=urls_to_scrape,\n",
    "    ranking_metric=ranking_metric_choice,\n",
    "    start=start_index,\n",
    "    end=end_index,\n",
    "    top_n=top_n_articles,\n",
    "    output_file=output_filename\n",
    ")\n",
    "print(len(final_df))\n",
    "\n",
    "final_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the other taxons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/List_of_edible_seeds'\n",
    "name = urls_to_scrape.split('/')[-1]\n",
    "tiny = False\n",
    "original_data_filename = f\"{name}.json\" if not tiny else f\"{name}_tiny.json\"\n",
    "scraped_data_filename = f\"{name}_scraped.json\"\n",
    "\n",
    "def filter_img_links(link):\n",
    "    return '' if '.svg' in link else link\n",
    "\n",
    "def get_image_and_text_from_wikipedia(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    taxobox = soup.find('table', {'class': 'infobox biota'})\n",
    "    img_link = ''\n",
    "    if taxobox:\n",
    "        img = taxobox.find('img')\n",
    "        if img and img.get('src'):\n",
    "            img_link = 'https:' + img['src'] if img['src'].startswith('//') else img['src']\n",
    "    if not img_link:\n",
    "        img = soup.find('img')\n",
    "        if img and img.get('src'):\n",
    "            img_link = 'https:' + img['src'] if img['src'].startswith('//') else img['src']\n",
    "    img_link = filter_img_links(img_link)\n",
    "    \n",
    "    text = \"No description available\"\n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    if content:\n",
    "        paragraphs = content.find_all('p', recursive=False)\n",
    "        for paragraph in paragraphs:\n",
    "            for sup in paragraph.find_all('sup'):\n",
    "                sup.decompose()\n",
    "            text_content = paragraph.get_text(separator=' ', strip=True)\n",
    "            if text_content:\n",
    "                text_content = re.sub(r'\\s+([.,!?;:])', r'\\1', text_content)\n",
    "                text_content = re.sub(r'\\s{2,}', ' ', text_content)\n",
    "                text = text_content\n",
    "                break\n",
    "    return img_link, text\n",
    "\n",
    "def get_taxon_data(taxon_name):\n",
    "    link = f\"https://en.wikipedia.org/wiki/{taxon_name.replace(' ', '_')}\"\n",
    "    img_link, text = get_image_and_text_from_wikipedia(link)\n",
    "    return {\n",
    "        'name': taxon_name,\n",
    "        'link': link,\n",
    "        'img_link': img_link,\n",
    "        'text': text\n",
    "    }\n",
    "\n",
    "df_original = pd.read_json(original_data_filename, orient='records', lines=True)\n",
    "df_original.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "taxa_columns = ['kingdom', 'infrakingdom', 'superdivision', 'division', 'subdivision', 'order', 'family', 'genus']\n",
    "# taxa_columns = ['superkingdom', 'kingdom', 'subkingdom', 'infrakingdom','superphylum', 'phylum', 'subphylum', 'superclass', 'class', 'subclass', 'infraclass', 'order', 'family', 'genus', 'species']\n",
    "unique_taxa = set()\n",
    "for col in taxa_columns:\n",
    "    unique_taxa.update(df_original[col].dropna().unique())\n",
    "\n",
    "scraped_data = []\n",
    "for taxon in sorted(unique_taxa):\n",
    "    taxon = str(taxon).strip()\n",
    "    if taxon:\n",
    "        data = get_taxon_data(taxon)\n",
    "        scraped_data.append(data)\n",
    "\n",
    "df_scraped = pd.DataFrame(scraped_data)\n",
    "df_scraped.to_json(scraped_data_filename, orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bokeh.models import ColumnDataSource, HoverTool, TapTool, CustomJS, WheelZoomTool, ResetTool, PanTool\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "from bokeh.io import curdoc\n",
    "\n",
    "tab_title = 'Edible Plants'\n",
    "# urls_to_scrape = 'https://en.wikipedia.org/wiki/Plants_used_as_herbs_or_spices'\n",
    "name = urls_to_scrape.split('/')[-1]\n",
    "name = 'edible_plants'\n",
    "original_data_filename = f\"{name}.json\" if not tiny else f\"{name}_tiny.json\"\n",
    "scraped_data_filename = f\"{name}_scraped.json\"\n",
    "output_plot_filename = f\"{name}.html\" if not tiny else f\"{name}_tiny.html\"\n",
    "\n",
    "rankings = [\n",
    "    'kingdom',\n",
    "    'division',\n",
    "    'order',\n",
    "    'family',\n",
    "    'genus',\n",
    "    'species'\n",
    "]\n",
    "\n",
    "root_taxon_name = 'Plantae'\n",
    "root_taxon_rank = 'kingdom'\n",
    "plot_height = 60000\n",
    "y_offset_internal = 0.5\n",
    "y_offset_multiplier = 0.07\n",
    "x_range_offset = 2\n",
    "y_range_offset_max = 12\n",
    "y_range_offset_min = 1\n",
    "text_multiplier = 1\n",
    "min_text = 15\n",
    "\n",
    "horizontal_lines_manual = [\n",
    "]\n",
    "\n",
    "vertical_lines_manual = [\n",
    "]\n",
    "\n",
    "nodes_manual = [\n",
    "    [0.1, 2756, 4, \"edge\", \"Go Back\", \"images/taxotree4.png\", \"\", \"https://kochjar.me/trees\"],\n",
    "    [3, 2756, 26, \"edge\", \"Edible Plants\", \"images/spongebob.png\", \"ive actually heard ppl sad this... this is sadly not a strawman\", \"https://www.youtube.com/watch?v=r7iT5GM3MgM\"],\n",
    "    [3, 2752, 14, \"edge\", \"This is tree containing species from all other edible plant trees\\nFor the sources check out the other trees.\\nClick \\\"Go Back\\\" to see them\\nIf you encounter lag, try using Chromium.\", \"images/large_tree.jpg\", \"I like big trees, they can't runaway from hugs >:3\", \"https://en.wikipedia.org/wiki/General_Sherman_(tree)\"],\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, name, depth, data=None):\n",
    "        self.name = name\n",
    "        self.depth = depth\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "        self.size = 0\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "def extract_species(name):\n",
    "    parts = re.split(r'[.\\s]+', name)\n",
    "    return parts[1] if len(parts) > 1 else name\n",
    "\n",
    "def filter_img_links(link):\n",
    "    return '' if '.svg' in link else link\n",
    "\n",
    "def make_tree(rank_name, rank_level, df_original, df_scraped):\n",
    "    if rank_level >= len(rankings):\n",
    "        return None\n",
    "    if rankings[rank_level] == 'genus':\n",
    "        species_info_list = sorted(\n",
    "            df_original[df_original[\"genus\"] == rank_name][\"species_info\"].tolist(),\n",
    "            key=lambda x: x['species_name'],\n",
    "            reverse=True\n",
    "        )\n",
    "        if not species_info_list:\n",
    "            return None\n",
    "        scraped_row = df_scraped[df_scraped['name'] == rank_name]\n",
    "        if not scraped_row.empty:\n",
    "            node_data = {\n",
    "                'name': rank_name,\n",
    "                'link': scraped_row.iloc[0]['link'],\n",
    "                'img_link': scraped_row.iloc[0]['img_link'],\n",
    "                'text': scraped_row.iloc[0]['text']\n",
    "            }\n",
    "        else:\n",
    "            node_data = {}\n",
    "        node = TreeNode(rank_name, rank_level, node_data)\n",
    "        for species_info in species_info_list:\n",
    "            node.children.append(TreeNode(\n",
    "                species_info['species_name'],\n",
    "                rank_level + 1,\n",
    "                species_info\n",
    "            ))\n",
    "    else:\n",
    "        scraped_row = df_scraped[df_scraped['name'] == rank_name]\n",
    "        if not scraped_row.empty:\n",
    "            node_data = {\n",
    "                'name': rank_name,\n",
    "                'link': scraped_row.iloc[0]['link'],\n",
    "                'img_link': scraped_row.iloc[0]['img_link'],\n",
    "                'text': scraped_row.iloc[0]['text']\n",
    "            }\n",
    "        else:\n",
    "            node_data = {}\n",
    "        node = TreeNode(rank_name, rank_level, node_data)\n",
    "        next_rank = rankings[rank_level + 1]\n",
    "        subranks = sorted(\n",
    "            df_original[df_original[rankings[rank_level]] == rank_name][next_rank].dropna().unique().tolist(),\n",
    "            reverse=True\n",
    "        )\n",
    "        for subrank_name in subranks:\n",
    "            if subrank_name:\n",
    "                child_node = make_tree(subrank_name, rank_level + 1, df_original, df_scraped)\n",
    "                if child_node is not None:\n",
    "                    node.children.append(child_node)\n",
    "    if not node.children:\n",
    "        return None\n",
    "    return node\n",
    "\n",
    "def assign_coordinates(tree):\n",
    "    def tree_size(node):\n",
    "        if not node.children:\n",
    "            return 1\n",
    "        return sum(tree_size(child) for child in node.children) + 1\n",
    "\n",
    "    def tree_depth(node):\n",
    "        if not node.children:\n",
    "            return 0\n",
    "        return 1 + max(tree_depth(child) for child in node.children)\n",
    "\n",
    "    def tree_leaves(node):\n",
    "        if not node.children:\n",
    "            return [node]\n",
    "        leaves = []\n",
    "        for child in node.children:\n",
    "            leaves.extend(tree_leaves(child))\n",
    "        return leaves\n",
    "\n",
    "    def tree_at_depth(node, depth):\n",
    "        if depth == 0:\n",
    "            return [node]\n",
    "        elif not node.children:\n",
    "            return []\n",
    "        nodes = []\n",
    "        for child in node.children:\n",
    "            nodes.extend(tree_at_depth(child, depth - 1))\n",
    "        return nodes\n",
    "\n",
    "    def leaf_coords(node, current_y, coords):\n",
    "        if not node.children:\n",
    "            coords.append((node, current_y[0]))\n",
    "            current_y[0] += 1\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                leaf_coords(child, current_y, coords)\n",
    "            current_y[0] += 1\n",
    "\n",
    "    def node_coords(node, coords):\n",
    "        if node.children:\n",
    "            children_coords = [child.y for child in node.children]\n",
    "            node.y = sum(children_coords) / len(children_coords)\n",
    "            coords.append((node, node.x, node.y))\n",
    "\n",
    "    current_y = [0]\n",
    "    depth = tree_depth(tree)\n",
    "\n",
    "    leaf_coords_list = []\n",
    "    leaf_coords(tree, current_y, leaf_coords_list)\n",
    "\n",
    "    for node, y in leaf_coords_list:\n",
    "        node.x = depth\n",
    "        node.y = y\n",
    "\n",
    "    for d in range(depth - 1, -1, -1):\n",
    "        nodes_at_depth = tree_at_depth(tree, d)\n",
    "        for node in nodes_at_depth:\n",
    "            node.x = d\n",
    "            node_coords(node, [])\n",
    "\n",
    "    return tree\n",
    "\n",
    "def calculate_subtree_sizes(node):\n",
    "    if not node.children:\n",
    "        node.size = 1\n",
    "    else:\n",
    "        node.size = 0\n",
    "        for child in node.children:\n",
    "            calculate_subtree_sizes(child)\n",
    "            node.size += child.size\n",
    "\n",
    "def collect_nodes_edges(node, nodes_list, edges_list):\n",
    "    nodes_list.append(node)\n",
    "    for child in node.children:\n",
    "        edges_list.append((node, child))\n",
    "        collect_nodes_edges(child, nodes_list, edges_list)\n",
    "\n",
    "def draw_tree_bokeh(nodes_list, edges_list, manual_lines, manual_nodes):\n",
    "    placeholder_image = \"https://via.placeholder.com/150\"\n",
    "    max_depth = max(node.x for node in nodes_list + [\n",
    "        TreeNode(n[3], n[0], {'link': n[7], 'img_link': n[5], 'text': n[6]}) for n in manual_nodes\n",
    "    ])\n",
    "    \n",
    "    p = figure(\n",
    "        title=\"\",\n",
    "        background_fill_color='black',\n",
    "        border_fill_color='black',\n",
    "        outline_line_color='black',\n",
    "        sizing_mode=\"stretch_width\",\n",
    "        height=plot_height,\n",
    "        x_range=(0, max_depth + x_range_offset),\n",
    "        y_range=(-y_range_offset_min, max(node.y for node in nodes_list) + y_range_offset_max)\n",
    "    )\n",
    "    \n",
    "    p.toolbar_location = \"right\"\n",
    "    p.toolbar_sticky = True\n",
    "    \n",
    "    xs_lines = []\n",
    "    ys_lines = []\n",
    "    line_widths = []\n",
    "    for parent, child in edges_list:\n",
    "        size_multiplier = (max_depth - parent.x)\n",
    "        line_thickness = 3 * size_multiplier + 2\n",
    "        xs_lines.append([parent.x - 1/450 * (3*(max_depth - parent.x) + 2), child.x])\n",
    "        ys_lines.append([parent.y, parent.y])\n",
    "        line_widths.append(line_thickness)\n",
    "        xs_lines.append([child.x, child.x])\n",
    "        ys_lines.append([parent.y, child.y])\n",
    "        line_widths.append(line_thickness)\n",
    "        if not child.children:\n",
    "            xs_lines.append([child.x, child.x + 0.5])\n",
    "            ys_lines.append([child.y, child.y])\n",
    "            line_widths.append(2)\n",
    "    \n",
    "    labels_data_internal = {\n",
    "        'x': [], 'y': [], 'text': [], 'link': [], 'img': [],\n",
    "        'text_size': [], 'original_text_size': [], 'depth': [], 'wikitext': [], 'text_align': []\n",
    "    }\n",
    "    labels_data_leaf = {\n",
    "        'x': [], 'y': [], 'text': [], 'link': [], 'img': [],\n",
    "        'text_size': [], 'original_text_size': [], 'depth': [], 'wikitext': [], 'text_align': []\n",
    "    }\n",
    "    \n",
    "    for node in nodes_list:\n",
    "        size_multiplier = (max_depth - node.x)\n",
    "        text_size = size_multiplier * text_multiplier + min_text\n",
    "        if node.children:\n",
    "            labels_data_internal['x'].append(node.x + 0.5)\n",
    "            labels_data_internal['y'].append(node.y + y_offset_internal + size_multiplier * y_offset_multiplier)\n",
    "            labels_data_internal['depth'].append(node.x)\n",
    "            labels_data_internal['text_size'].append(f\"{text_size}px\")\n",
    "            labels_data_internal['original_text_size'].append(f\"{text_size}px\")\n",
    "            labels_data_internal['text'].append(node.name)\n",
    "            labels_data_internal['link'].append(node.data.get('link', ''))\n",
    "            labels_data_internal['img'].append(node.data.get('img_link', placeholder_image))\n",
    "            labels_data_internal['wikitext'].append(node.data.get('text', \"\"))\n",
    "            labels_data_internal['text_align'].append('center')\n",
    "        elif node.x == max_depth:\n",
    "            species_info = node.data\n",
    "            labels_data_leaf['x'].append(node.x + 0.52)\n",
    "            labels_data_leaf['y'].append(node.y)\n",
    "            labels_data_leaf['depth'].append(node.x)\n",
    "            labels_data_leaf['text_size'].append(f\"{text_size}px\")\n",
    "            labels_data_leaf['original_text_size'].append(f\"{text_size}px\")\n",
    "            labels_data_leaf['text'].append(f\"{species_info.get('species_name', '')} ── {species_info.get('title', '')}\")\n",
    "            labels_data_leaf['link'].append(species_info.get('link', ''))\n",
    "            labels_data_leaf['img'].append(species_info.get('img_link', placeholder_image))\n",
    "            labels_data_leaf['wikitext'].append(species_info.get('text', \"\"))\n",
    "            labels_data_leaf['text_align'].append('left') \n",
    "     \n",
    "    manual_with_hover = [n for n in manual_nodes if n[5] or n[6]]\n",
    "    manual_without_hover = [n for n in manual_nodes if not n[5] and not n[6]]\n",
    "    \n",
    "    labels_manual_with_hover = {\n",
    "        'x': [], 'y': [], 'text': [], 'link': [], 'img': [],\n",
    "        'text_size': [], 'original_text_size': [], 'depth': [], 'wikitext': [], 'text_align': []\n",
    "    }\n",
    "    labels_manual_without_hover = {\n",
    "        'x': [], 'y': [], 'text': [], 'link': [], 'img': [],\n",
    "        'text_size': [], 'original_text_size': [], 'depth': [], 'wikitext': [], 'text_align': []\n",
    "    }\n",
    "    \n",
    "    for node in manual_with_hover:\n",
    "        x, y, base_size, type_, text, hover_img, hover_text, link = node\n",
    "        size_multiplier = (max_depth - x)\n",
    "        text_size = base_size + size_multiplier * 4\n",
    "        labels_manual_with_hover['x'].append(x)\n",
    "        if type_ == \"node\":\n",
    "            labels_manual_with_hover['y'].append(y + (max_depth - x) * y_offset_internal)\n",
    "        else:\n",
    "            labels_manual_with_hover['y'].append(y)\n",
    "        labels_manual_with_hover['depth'].append(x)\n",
    "        labels_manual_with_hover['text_size'].append(f\"{text_size}px\")\n",
    "        labels_manual_with_hover['original_text_size'].append(f\"{text_size}px\")\n",
    "        labels_manual_with_hover['text'].append(text)\n",
    "        labels_manual_with_hover['link'].append(link)\n",
    "        labels_manual_with_hover['img'].append(hover_img if hover_img else placeholder_image)\n",
    "        labels_manual_with_hover['wikitext'].append(hover_text)\n",
    "        if type_ == \"edge\":\n",
    "            labels_manual_with_hover['text_align'].append('left')\n",
    "        else:\n",
    "            labels_manual_with_hover['text_align'].append('center')\n",
    "    \n",
    "    for node in manual_without_hover:\n",
    "        x, y, base_size, type_, text, hover_img, hover_text, link = node\n",
    "        size_multiplier = (max_depth - x)\n",
    "        text_size = base_size + size_multiplier * 4\n",
    "        labels_manual_without_hover['x'].append(x)\n",
    "        if type_ == \"node\":\n",
    "            labels_manual_without_hover['y'].append(y + (max_depth - x) * y_offset_internal)\n",
    "        else:\n",
    "            labels_manual_without_hover['y'].append(y)\n",
    "        labels_manual_without_hover['depth'].append(x)\n",
    "        labels_manual_without_hover['text_size'].append(f\"{text_size}px\")\n",
    "        labels_manual_without_hover['original_text_size'].append(f\"{text_size}px\")\n",
    "        labels_manual_without_hover['text'].append(text)\n",
    "        labels_manual_without_hover['link'].append(link)\n",
    "        labels_manual_without_hover['img'].append('')\n",
    "        labels_manual_without_hover['wikitext'].append('')\n",
    "        if type_ == \"edge\":\n",
    "            labels_manual_without_hover['text_align'].append('left')\n",
    "        else:\n",
    "            labels_manual_without_hover['text_align'].append('center')\n",
    "\n",
    "    labels_source_internal = ColumnDataSource(data=labels_data_internal)\n",
    "    labels_source_leaf = ColumnDataSource(data=labels_data_leaf)\n",
    "    labels_manual_with_hover_source = ColumnDataSource(data=labels_manual_with_hover)\n",
    "    labels_manual_without_hover_source = ColumnDataSource(data=labels_manual_without_hover)\n",
    "\n",
    "    lines_source = ColumnDataSource(data=dict(\n",
    "        xs=xs_lines,\n",
    "        ys=ys_lines,\n",
    "        line_widths=line_widths,\n",
    "        original_line_widths=line_widths.copy()\n",
    "    ))\n",
    "    p.multi_line(xs='xs', ys='ys', source=lines_source, line_color=\"white\", line_width='line_widths')\n",
    "\n",
    "    internal_text_renderer = p.text(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        text='text',\n",
    "        source=labels_source_internal,\n",
    "        text_color='white',\n",
    "        text_font_size='text_size',\n",
    "        text_baseline='middle',\n",
    "        text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    leaf_text_renderer = p.text(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        text='text',\n",
    "        source=labels_source_leaf,\n",
    "        text_color='white',\n",
    "        text_font_size='text_size',\n",
    "        text_baseline='middle',\n",
    "        text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    manual_with_hover_renderer = p.text(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        text='text',\n",
    "        source=labels_manual_with_hover_source,\n",
    "        text_color='white',\n",
    "        text_font_size='text_size',\n",
    "        text_baseline='middle',\n",
    "        text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    manual_without_hover_renderer = p.text(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        text='text',\n",
    "        source=labels_manual_without_hover_source,\n",
    "        text_color='white',\n",
    "        text_font_size='text_size',\n",
    "        text_baseline='middle',\n",
    "        text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    TOOLTIPS = \"\"\"\n",
    "    <div style=\"width: 350px; background-color: rgba(221, 221, 221, 1); padding: 10px;\">\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"@img\" alt=\"\"\n",
    "                style=\"\n",
    "                    max-height: 300px; \n",
    "                    max-width: 300px; \n",
    "                    height: auto; \n",
    "                    width: auto; \n",
    "                    display: block; \n",
    "                    margin: 0 auto; \n",
    "                    @{img == '' ? 'display:none;' : ''};\"\n",
    "                border=\"0\">\n",
    "        </div>\n",
    "        <div style=\"text-align: center; @{wikitext == '' ? 'display:none;' : ''}\">\n",
    "            <span style=\"font-size: 16px; color: #000000;\">@wikitext</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    hover_tool_internal = HoverTool(\n",
    "        tooltips=TOOLTIPS,\n",
    "        renderers=[internal_text_renderer, leaf_text_renderer],\n",
    "        point_policy=\"follow_mouse\"\n",
    "    )\n",
    "    \n",
    "    hover_tool_manual = HoverTool(\n",
    "        tooltips=TOOLTIPS,\n",
    "        renderers=[manual_with_hover_renderer],\n",
    "        point_policy=\"follow_mouse\"\n",
    "    )\n",
    "    \n",
    "    p.add_tools(hover_tool_internal, hover_tool_manual)\n",
    "    \n",
    "    callback = CustomJS(args=dict(\n",
    "        source_internal=labels_source_internal, \n",
    "        source_leaf=labels_source_leaf,\n",
    "        source_manual_with_hover=labels_manual_with_hover_source,\n",
    "        source_manual_without_hover=labels_manual_without_hover_source\n",
    "    ), code=\"\"\"\n",
    "        const selected_internal = source_internal.selected.indices;\n",
    "        const selected_leaf = source_leaf.selected.indices;\n",
    "        const selected_manual_with = source_manual_with_hover.selected.indices;\n",
    "        const selected_manual_without = source_manual_without_hover.selected.indices;\n",
    "\n",
    "        if (selected_internal.length > 0) {\n",
    "            const index = selected_internal[0];\n",
    "            const url = source_internal.data['link'][index];\n",
    "            if (url) {\n",
    "                window.open(url, \"_blank\");\n",
    "            }\n",
    "        }\n",
    "        if (selected_leaf.length > 0) {\n",
    "            const index = selected_leaf[0];\n",
    "            const url = source_leaf.data['link'][index];\n",
    "            if (url) {\n",
    "                window.open(url, \"_blank\");\n",
    "            }\n",
    "        }\n",
    "        if (selected_manual_with.length > 0) {\n",
    "            const index = selected_manual_with[0];\n",
    "            const url = source_manual_with_hover.data['link'][index];\n",
    "            if (url) {\n",
    "                window.open(url, \"_blank\");\n",
    "            }\n",
    "        }\n",
    "        if (selected_manual_without.length > 0) {\n",
    "            const index = selected_manual_without[0];\n",
    "            const url = source_manual_without_hover.data['link'][index];\n",
    "            if (url) {\n",
    "                window.open(url, \"_blank\");\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Clear selection\n",
    "        source_internal.selected.indices = [];\n",
    "        source_leaf.selected.indices = [];\n",
    "        source_manual_with_hover.selected.indices = [];\n",
    "        source_manual_without_hover.selected.indices = [];\n",
    "    \"\"\")\n",
    "    \n",
    "    tap_tool = TapTool(callback=callback, renderers=[\n",
    "        internal_text_renderer, \n",
    "        leaf_text_renderer, \n",
    "        manual_with_hover_renderer, \n",
    "        manual_without_hover_renderer\n",
    "    ])\n",
    "    p.add_tools(tap_tool)\n",
    "    \n",
    "\n",
    "    initial_x_range = p.x_range.end - p.x_range.start\n",
    "\n",
    "    zoom_callback = CustomJS(args=dict(\n",
    "    labels_source_internal=labels_source_internal,\n",
    "    labels_source_leaf=labels_source_leaf,\n",
    "    labels_manual_with_hover_source=labels_manual_with_hover_source,\n",
    "    labels_manual_without_hover_source=labels_manual_without_hover_source,\n",
    "    lines_source=lines_source,\n",
    "    plot=p,\n",
    "    initial_x_range=initial_x_range\n",
    "), code=\"\"\"\n",
    "    const x_range = plot.x_range.end - plot.x_range.start;\n",
    "\n",
    "    // Scale Text Sizes for Internal Nodes\n",
    "    const labels_data_internal = labels_source_internal.data;\n",
    "    const depths_internal = labels_data_internal['depth'];\n",
    "    const n_labels_internal = depths_internal.length;\n",
    "    for (let i = 0; i < n_labels_internal; i++) {\n",
    "        const depth = depths_internal[i];\n",
    "        const threshold = 35 - depth * 3;\n",
    "        labels_data_internal['text_size'][i] = (x_range > threshold) ? '0px' : labels_data_internal['original_text_size'][i];\n",
    "    }\n",
    "    labels_source_internal.change.emit();\n",
    "\n",
    "    // Scale Text Sizes for Leaf Nodes\n",
    "    const labels_data_leaf = labels_source_leaf.data;\n",
    "    const depths_leaf = labels_data_leaf['depth'];\n",
    "    const n_labels_leaf = depths_leaf.length;\n",
    "    for (let i = 0; i < n_labels_leaf; i++) {\n",
    "        const depth = depths_leaf[i];\n",
    "        const threshold = 35 - depth * 3;\n",
    "        labels_data_leaf['text_size'][i] = (x_range > threshold) ? '0px' : labels_data_leaf['original_text_size'][i];\n",
    "    }\n",
    "    labels_source_leaf.change.emit();\n",
    "\n",
    "    // Scale Text Sizes for Manual Nodes with Hover\n",
    "    const labels_manual_with = labels_manual_with_hover_source.data;\n",
    "    const depths_manual_with = labels_manual_with['depth'];\n",
    "    const n_manual_with = depths_manual_with.length;\n",
    "    for (let i = 0; i < n_manual_with; i++) {\n",
    "        const depth = depths_manual_with[i];\n",
    "        const threshold = 35;\n",
    "        labels_manual_with['text_size'][i] = (x_range > threshold) ? '0px' : labels_manual_with['original_text_size'][i];\n",
    "    }\n",
    "    labels_manual_with_hover_source.change.emit();\n",
    "\n",
    "    // Scale Text Sizes for Manual Nodes without Hover\n",
    "    const labels_manual_without = labels_manual_without_hover_source.data;\n",
    "    const depths_manual_without = labels_manual_without['depth'];\n",
    "    const n_manual_without = depths_manual_without.length;\n",
    "    for (let i = 0; i < n_manual_without; i++) {\n",
    "        const depth = depths_manual_without[i];\n",
    "        const threshold = 35;\n",
    "        labels_manual_without['text_size'][i] = (x_range > threshold) ? '0px' : labels_manual_without['original_text_size'][i];\n",
    "    }\n",
    "    labels_manual_without_hover_source.change.emit();\n",
    "\n",
    "    // Scale Line Widths\n",
    "    const lines_data = lines_source.data;\n",
    "    const line_widths = lines_data['line_widths'];\n",
    "    const original_line_widths = lines_data['original_line_widths'];\n",
    "    const n_lines = line_widths.length;\n",
    "\n",
    "    // Calculate scaling factor\n",
    "    const scaling_factor = initial_x_range / x_range;\n",
    "    const min_scaling_factor = 0.1;\n",
    "    const max_scaling_factor = 1;\n",
    "\n",
    "    // Apply scaling factor to line widths\n",
    "    const line_scaling_factor = Math.max(min_scaling_factor, Math.min(max_scaling_factor, scaling_factor));\n",
    "    for (let i = 0; i < n_lines; i++) {\n",
    "        line_widths[i] = original_line_widths[i] * line_scaling_factor;\n",
    "    }\n",
    "    lines_source.change.emit();\n",
    "\"\"\")\n",
    "\n",
    "    p.x_range.js_on_change('start', zoom_callback)\n",
    "    p.x_range.js_on_change('end', zoom_callback)\n",
    "    p.y_range.js_on_change('start', zoom_callback)\n",
    "    p.y_range.js_on_change('end', zoom_callback)\n",
    "\n",
    "    p.grid.visible = False\n",
    "    p.axis.visible = False\n",
    "    \n",
    "    wheel_zoom_tool = WheelZoomTool()\n",
    "    reset_tool = ResetTool()\n",
    "    pan_tool = PanTool()\n",
    "    \n",
    "    p.toolbar.tools = [\n",
    "        pan_tool, \n",
    "        wheel_zoom_tool, \n",
    "        tap_tool, \n",
    "        hover_tool_internal, \n",
    "        hover_tool_manual, \n",
    "        reset_tool\n",
    "    ]\n",
    "    # p.toolbar.active_scroll = wheel_zoom_tool\n",
    "    p.toolbar.active_drag = pan_tool\n",
    "    p.toolbar.logo = None\n",
    "    \n",
    "    manual_lines_source = ColumnDataSource(data=dict(\n",
    "        xs=[[line[0][0], line[1][0]] for line in horizontal_lines_manual + vertical_lines_manual],\n",
    "        ys=[[line[0][1], line[1][1]] for line in horizontal_lines_manual + vertical_lines_manual],\n",
    "        line_widths=[3 * (max_depth - line[0][0]) + 2 for line in horizontal_lines_manual + vertical_lines_manual],\n",
    "        original_line_widths=[3 * (max_depth - line[0][0]) + 2 for line in horizontal_lines_manual + vertical_lines_manual]\n",
    "    ))\n",
    "    p.multi_line(xs='xs', ys='ys', source=manual_lines_source, line_color=\"white\", line_width='line_widths')\n",
    "    \n",
    "    output_file(output_plot_filename, title=tab_title)\n",
    "    show(p)\n",
    "\n",
    "def draw_tree():\n",
    "    output_notebook()\n",
    "    df_original = pd.read_json(original_data_filename, orient='records', lines=True)\n",
    "    df_original.dropna(subset=['species'], inplace=True)\n",
    "    df_original.drop_duplicates(subset=['species', 'title'], inplace=True)\n",
    "    df_original.fillna('', inplace=True)\n",
    "    df_original['img_link'] = df_original['img_link'].apply(filter_img_links)\n",
    "    df_original['species_info'] = df_original.apply(lambda row: {\n",
    "        'species_name': extract_species(row['species']),\n",
    "        'title': row['title'],\n",
    "        'link': row['link'],\n",
    "        'img_link': row['img_link'],\n",
    "        'text': row['text']\n",
    "    }, axis=1)\n",
    "    \n",
    "    df_scraped = pd.read_json(scraped_data_filename, orient='records', lines=True)\n",
    "    df_scraped.fillna('', inplace=True)\n",
    "    \n",
    "    \n",
    "    root_node = make_tree(root_taxon_name, rankings.index(root_taxon_rank), df_original, df_scraped)\n",
    "    if not root_node:\n",
    "        print(\"No valid branches found leading to species.\")\n",
    "        return\n",
    "    \n",
    "    calculate_subtree_sizes(root_node)\n",
    "    assign_coordinates(root_node)\n",
    "    \n",
    "    nodes_list = []\n",
    "    edges_list = []\n",
    "    collect_nodes_edges(root_node, nodes_list, edges_list)\n",
    "    \n",
    "    draw_tree_bokeh(nodes_list, edges_list, horizontal_lines_manual, nodes_manual)\n",
    "\n",
    "draw_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing home page\n",
    "\n",
    "yes i drew this manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.models import ColumnDataSource, HoverTool, TapTool, CustomJS, WheelZoomTool, ResetTool, PanTool\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "placeholder_image = \"https://via.placeholder.com/150\"\n",
    "y_offset_internal = 0.3\n",
    "y_offset_multiplier = 0.125\n",
    "text_multiplier = 4\n",
    "default_min_text_size = 14\n",
    "plot_width = 1920\n",
    "plot_height = 900\n",
    "\n",
    "horizontal_lines = [\n",
    "    [(0, 0), (1, 0)],\n",
    "    [(1, -2.75), (2, -2.75)],\n",
    "    [(1, 2.75), (2, 2.75)],\n",
    "    [(2, 2.75), (3, 2.75)],\n",
    "    [(2, -2.75), (3, -2.75)],\n",
    "    [(3, 6.75), (3.5, 6.75)],\n",
    "    [(3, 5.75), (3.5, 5.75)],\n",
    "    [(3, 4.75), (3.5, 4.75)],\n",
    "    [(3, 3.75), (3.5, 3.75)],\n",
    "    [(3, 2.75), (3.5, 2.75)],\n",
    "    [(3, 1.75), (3.5, 1.75)],\n",
    "    [(3, 0.75), (3.5, 0.75)],\n",
    "    [(3, -0.25), (3.5, -0.25)],\n",
    "    [(3, -1.25), (3.5, -1.25)],\n",
    "    [(3, -2.25), (3.5, -2.25)],\n",
    "    [(3, -3.25), (3.5, -3.25)],\n",
    "]\n",
    "\n",
    "vertical_lines = [\n",
    "    [(1, -2.75), (1, 2.75)],\n",
    "    [(3, -1.25), (3, 6.75)],\n",
    "    [(3, -2.25), (3, -3.25)],\n",
    "]\n",
    "\n",
    "# kochjar.me/spacer actually useful???\n",
    "squirrel = \"\"\" \n",
    "\n",
    "                                                             _ \n",
    "                                                     . - ' `   ` } \n",
    "                                     _ . / )       /               } \n",
    "                                 . ' o       \\ \\   |               } \n",
    "                                 ' . _ _ _ . ' ` . \\ \\         { ` \n",
    "                                 / ` \\ \\ _ /     ,   ` .         } \n",
    "                                 \\ \\ = '   . - '       _ ` \\ \\     { \n",
    "                                   ` ' ` ;               ` ,     } \n",
    "                                         _ \\ \\               ;     } \n",
    "                                       / _ _ ` ; . . . - ' - - ' \n",
    " \"\"\"\n",
    "\n",
    "\n",
    "nodes = [\n",
    "    [0.05, 12.5, 6, \"edge\", \"Source Code\", \"\", \"\", \"https://github.com/jan-mate/taxo_trees\"],\n",
    "    [1.5, 12.5, 36, \"edge\", \"Wikipedia Taxonomic Trees\", \"\", \"\", \"\"],\n",
    "    [1.5, 9.5, 18, \"edge\", \"12 interactive wiki taxonomic trees. You can\\n\\\n",
    "hover and click on all* words to explore! You can\\nalso zoom and move the trees around, for more\\noptions click the buttons in top-right corner.\\n\\\n",
    "All the data is from Wikipedia, and it certaintly\\ncontains errors!\\nChromium is recommended for this.\", \"\", \"\", \"\"],\n",
    "\n",
    "    [0.5, 0, 15, \"node\", \"taxonomic trees\", \"images/taxotree4.png\", \"Interactive Taxonomic trees generated from Wikipedia\", \"https://kochjar.me/trees\"],\n",
    "    [1.5, 2.75, 15, \"node\", \"plantae\", \"images/forsthaven.jpg\", \"Forst Botanisk Have in Charlottenlund is my fav collection of plants, go there :)\", \"https://en.wikipedia.org/wiki/Plant\"],\n",
    "    [1.5, -2.75, 15, \"node\", \"animalia\", \"images/redpanda.jpeg\", \"red pandas so much cooler than fake pandas. if u prefer fake pandas ur not invited to my birthday party\", \"https://en.wikipedia.org/wiki/Animal\"],\n",
    "    [2.5, 2.75, 15, \"node\", \"edible\", \"images/plants.jpg\", \"btw you can click here and see a huge tree of all edible plants :)\", \"/edible_plants\"],\n",
    "    [2.5, -2.75, 15, \"node\", \"danish\", \"images/dk.png\", \"The 3rd best Nordic country (🇩🇰🇸🇪🇳🇴🇮🇸🇫🇮🇪🇪🇬🇱🇫🇴🇷🇺🇰🇵🇲🇰)\", \"https://en.wikipedia.org/wiki/Denmark\"],\n",
    "    [3.52, 6.75, 15, \"edge\", \"fruit\", \"images/squirrel_mango.jpg\", \"Unlike much foods, fruit evolved to taste good for us, and we evolved to like it. Therefore it makes sense the tastiest food is fruit.\", \"/fruit\"],\n",
    "    [3.52, 5.75, 15, \"edge\", \"veggies\", \"images/squirrel_pumpkin.png\", \"My favorite thing about being adult is you can eat veggies every day! And no one will stop you!\", \"/veggies\"],\n",
    "    [3.52, 4.75, 15, \"edge\", \"nuts\", \"images/squirrel_fishing.png\", \"Top 10 nuts:\\n10. Ginkgo nuts\\n9.Chestnuts \\n8. Pecan \\n7. Hazelnut \\n6. Cashew \\n5. Brazil Nut\\n4\\n.Coconut \\n3. Peanut \\n2. Almond\\n1. Walnut\\n Honerable mention. cum\", \"/nuts\"],\n",
    "    [3.52, 3.75, 15, \"edge\", \"(pseudo)cereals\", \"images/squirrel_cereal.jpg\", \"Just add milk and you have soup\", \"/cereals\"],\n",
    "    [3.52, 2.75, 15, \"edge\", \"herbs ⋀ spices\", \"images/squirrel_chili.webp\", \"food must have been really bland back then since commited genocide to get spices\", \"/herbs_spices\"],\n",
    "    [3.52, 1.75, 15, \"edge\", \"flowers\", \"images/flower_squirrel.jpg\", \"don't forget to eat the flowers\", \"/flowers\"],\n",
    "    [3.52, 0.75, 15, \"edge\", \"forageable\", \"images/skeleton.jpg\", \"when you use this as ur foraging guide\", \"forageable\"],\n",
    "    [3.52, -0.25, 15, \"edge\", \"roots\", \"images/squareroot.jpg\", \"√\", \"/roots\"],\n",
    "    [3.52, -1.25, 15, \"edge\", \"leaf\", \"images/squirrel_eat_leaf.jpg\", \"i love spinach but like its much expensif and then u cook it, and it always leafs you wondering where 99% of it went :(\", \"/leaf\"],\n",
    "    [3.52, -2.25, 15, \"edge\", \"dinosaurs\", \"images/goose.jpg\", \"The birds of Denmark\", \"/dkbirds\"],\n",
    "    [3.52, -3.25, 15, \"edge\", \"titty animals\", \"images/squirrelske.jpg\", \"The mammals of Denmark\", \"/dkmammals\"],\n",
    "\n",
    "    [-15, 15, -60, \"node\", squirrel, \"images/bobo.jpg\", \"ascii art by Joan G. Stark\", \"https://www.youtube.com/watch?v=1CIMGTO6aFc\"],\n",
    "]\n",
    "\n",
    "\n",
    "def draw_manual_tree(horizontal_lines, vertical_lines, nodes, output_plot_filename=\"homepage.html\"):\n",
    "    nodes_df = pd.DataFrame(nodes, columns=['x', 'y', 'base_text_size', 'type', 'text', 'hover_img', 'hover_text', 'link'])\n",
    "    max_depth = max(max(line[0][0], line[1][0]) for line in horizontal_lines + vertical_lines)\n",
    "    \n",
    "    xs_lines, ys_lines, line_widths = [], [], []\n",
    "    for line in horizontal_lines + vertical_lines:\n",
    "        xs_lines.append([line[0][0], line[1][0]])\n",
    "        ys_lines.append([line[0][1], line[1][1]])\n",
    "        depth = line[0][0]\n",
    "        size_multiplier = max_depth - depth\n",
    "        line_widths.append(3 * size_multiplier + 2)\n",
    "    \n",
    "    lines_source = ColumnDataSource(data=dict(\n",
    "        xs=xs_lines,\n",
    "        ys=ys_lines,\n",
    "        line_widths=line_widths,\n",
    "        original_line_widths=line_widths.copy()\n",
    "    ))\n",
    "\n",
    "    nodes_with_hover = nodes_df[(nodes_df['hover_img'] != \"\") | (nodes_df['hover_text'] != \"\")]\n",
    "    nodes_without_hover = nodes_df[(nodes_df['hover_img'] == \"\") & (nodes_df['hover_text'] == \"\")]\n",
    "    \n",
    "    text_size_with_hover = [\n",
    "        f\"{base_size + (max_depth - x) * text_multiplier}px\" \n",
    "        for x, base_size in zip(nodes_with_hover['x'], nodes_with_hover['base_text_size'])\n",
    "    ]\n",
    "    text_size_without_hover = [\n",
    "        f\"{base_size + (max_depth - x) * text_multiplier}px\" \n",
    "        for x, base_size in zip(nodes_without_hover['x'], nodes_without_hover['base_text_size'])\n",
    "    ]\n",
    "    \n",
    "    y_offset_scaled_with_hover = [\n",
    "        (max_depth - x) * y_offset_multiplier + y_offset_internal if t == \"node\" else 0\n",
    "        for x, t in zip(nodes_with_hover['x'], nodes_with_hover['type'])\n",
    "    ]\n",
    "    y_offset_scaled_without_hover = [\n",
    "        (max_depth - x) * y_offset_multiplier + y_offset_internal if t == \"node\" else 0\n",
    "        for x, t in zip(nodes_without_hover['x'], nodes_without_hover['type'])\n",
    "    ]\n",
    "    \n",
    "    text_align_with_hover = nodes_with_hover['type'].apply(lambda t: 'left' if t == \"edge\" else 'center')\n",
    "    text_align_without_hover = nodes_without_hover['type'].apply(lambda t: 'left' if t == \"edge\" else 'center')\n",
    "    \n",
    "    labels_with_hover_source = ColumnDataSource(data=dict(\n",
    "        x=nodes_with_hover['x'], \n",
    "        y=nodes_with_hover['y'] + y_offset_scaled_with_hover,\n",
    "        base_y=nodes_with_hover['y'],\n",
    "        text=nodes_with_hover['text'], \n",
    "        hover_img=nodes_with_hover['hover_img'],\n",
    "        hover_text=nodes_with_hover['hover_text'], \n",
    "        link=nodes_with_hover['link'],\n",
    "        depth=nodes_with_hover['x'], \n",
    "        text_align=text_align_with_hover,\n",
    "        text_size=text_size_with_hover,\n",
    "        original_text_size=text_size_with_hover,\n",
    "        original_y_offset=y_offset_scaled_with_hover\n",
    "    ))\n",
    "    \n",
    "    labels_without_hover_source = ColumnDataSource(data=dict(\n",
    "        x=nodes_without_hover['x'], \n",
    "        y=nodes_without_hover['y'] + y_offset_scaled_without_hover,\n",
    "        base_y=nodes_without_hover['y'],\n",
    "        text=nodes_without_hover['text'], \n",
    "        hover_img=nodes_without_hover['hover_img'],\n",
    "        hover_text=nodes_without_hover['hover_text'], \n",
    "        link=nodes_without_hover['link'],\n",
    "        depth=nodes_without_hover['x'], \n",
    "        text_align=text_align_without_hover,\n",
    "        text_size=text_size_without_hover,\n",
    "        original_text_size=text_size_without_hover,\n",
    "        original_y_offset=y_offset_scaled_without_hover\n",
    "    ))\n",
    "    \n",
    "    p = figure(title=\"\",\n",
    "        background_fill_color='black',\n",
    "        border_fill_color='black',\n",
    "        outline_line_color='black',\n",
    "        sizing_mode=\"stretch_both\",\n",
    "        x_range=(0, max_depth + 0.35), y_range=(-5, 13))\n",
    "    \n",
    "    p.multi_line(xs='xs', ys='ys', source=lines_source, line_color=\"white\", line_width='line_widths')\n",
    "    \n",
    "    text_renderer_with_hover = p.text(\n",
    "        x='x', y='y', text='text', source=labels_with_hover_source,\n",
    "        text_color='white', text_font_size='text_size', text_baseline='middle', text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    text_renderer_without_hover = p.text(\n",
    "        x='x', y='y', text='text', source=labels_without_hover_source,\n",
    "        text_color='white', text_font_size='text_size', text_baseline='middle', text_align='text_align'\n",
    "    )\n",
    "    \n",
    "    callback = CustomJS(args=dict(source=labels_with_hover_source), code=\"\"\"\n",
    "        const selected_index = source.selected.indices[0];\n",
    "        if (selected_index != null) {\n",
    "            const url = source.data['link'][selected_index];\n",
    "            if (url) {\n",
    "                let newUrl;\n",
    "                if (url.startsWith('http://') || url.startsWith('https://')) {\n",
    "                    newUrl = url;\n",
    "                } else {\n",
    "                    const baseUrl = window.location.href.substring(0, window.location.href.lastIndexOf('/') + 1);\n",
    "                    newUrl = new URL(url, baseUrl).href;\n",
    "                }\n",
    "                window.open(newUrl, \"_blank\");\n",
    "            }\n",
    "        }\n",
    "        source.selected.indices = [];\n",
    "    \"\"\")\n",
    "\n",
    "    tap_tool = TapTool(callback=callback, renderers=[text_renderer_with_hover])\n",
    "    p.add_tools(tap_tool)\n",
    "    \n",
    "    TOOLTIPS = \"\"\"\n",
    "    <div style=\"width: 350px; background-color: rgba(221, 221, 221, 1); padding: 10px;\">\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"@hover_img\" alt=\"\"\n",
    "                style=\"display: block; margin: 0 auto; max-height: 300px; max-width: 300px; height: auto; width: auto;\" \n",
    "                border=\"0\">\n",
    "        </div>\n",
    "        <div style=\"text-align: center; margin-top: 15px;\">\n",
    "            <span style=\"font-size: 16px; color: #000000;\">@hover_text</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    hover_tool = HoverTool(tooltips=TOOLTIPS, renderers=[text_renderer_with_hover], point_policy=\"follow_mouse\")\n",
    "    p.add_tools(hover_tool)\n",
    "    \n",
    "    zoom_callback = CustomJS(args=dict(\n",
    "        labels_with_hover_source=labels_with_hover_source,\n",
    "        labels_without_hover_source=labels_without_hover_source,\n",
    "        lines_source=lines_source,\n",
    "        plot=p\n",
    "    ), code=\"\"\"\n",
    "        const x_range = plot.x_range.end - plot.x_range.start;\n",
    "        \n",
    "\n",
    "        const labels_with = labels_with_hover_source.data;\n",
    "        const depths_with = labels_with['depth'];\n",
    "        const n_labels_with = depths_with.length;\n",
    "        for (let i = 0; i < n_labels_with; i++) {\n",
    "            const depth = depths_with[i];\n",
    "            const threshold = 35 - depth * 3;\n",
    "            let text_size;\n",
    "            if (x_range > threshold) {\n",
    "                text_size = '0px';\n",
    "            } else {\n",
    "                text_size = labels_with['original_text_size'][i];\n",
    "            }\n",
    "            labels_with['text_size'][i] = text_size;\n",
    "            labels_with['y'][i] = labels_with['base_y'][i] + labels_with['original_y_offset'][i];\n",
    "        }\n",
    "        labels_with_hover_source.change.emit();\n",
    "        \n",
    "        \n",
    "        const labels_without = labels_without_hover_source.data;\n",
    "        const depths_without = labels_without['depth'];\n",
    "        const n_labels_without = depths_without.length;\n",
    "        for (let i = 0; i < n_labels_without; i++) {\n",
    "            const depth = depths_without[i];\n",
    "            const threshold = 5;\n",
    "            let text_size;\n",
    "            if (x_range > threshold) {\n",
    "                text_size = '0px';\n",
    "            } else {\n",
    "                text_size = labels_without['original_text_size'][i];\n",
    "            }\n",
    "            labels_without['text_size'][i] = text_size;\n",
    "            labels_without['y'][i] = labels_without['base_y'][i] + labels_without['original_y_offset'][i];\n",
    "        }\n",
    "        labels_without_hover_source.change.emit();\n",
    "    \n",
    "        \n",
    "        const lines_data = lines_source.data;\n",
    "        const line_widths = lines_data['line_widths'];\n",
    "        const original_line_widths = lines_data['original_line_widths'];\n",
    "        const n_lines = line_widths.length;\n",
    "        const scaling_factor = Math.max(0.1, Math.min(1, 800 / x_range));\n",
    "        for (let i = 0; i < n_lines; i++) {\n",
    "            const original_width = original_line_widths[i];\n",
    "            let new_width = original_width * scaling_factor;\n",
    "            line_widths[i] = new_width;\n",
    "        }\n",
    "        lines_source.change.emit();\n",
    "    \"\"\")\n",
    "    \n",
    "    p.x_range.js_on_change('start', zoom_callback)\n",
    "    p.x_range.js_on_change('end', zoom_callback)\n",
    "    p.y_range.js_on_change('start', zoom_callback)\n",
    "    p.y_range.js_on_change('end', zoom_callback)\n",
    "\n",
    "    p.grid.visible = False\n",
    "    p.axis.visible = False\n",
    "    \n",
    "    wheel_zoom_tool = WheelZoomTool()\n",
    "    reset_tool = ResetTool()\n",
    "    pan_tool = PanTool()\n",
    "\n",
    "    p.toolbar.tools = [pan_tool, wheel_zoom_tool, tap_tool, hover_tool, reset_tool]\n",
    "\n",
    "    p.toolbar.logo = None\n",
    "    p.toolbar.active_scroll = wheel_zoom_tool\n",
    "    p.toolbar.active_drag = pan_tool\n",
    "    \n",
    "    output_file(output_plot_filename)\n",
    "    show(p)\n",
    "\n",
    "draw_manual_tree(horizontal_lines, vertical_lines, nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def combine_jsons(json_files, output_main_file, output_scraped_file):\n",
    "    combined_main_df = pd.DataFrame()\n",
    "    combined_scraped_df = pd.DataFrame()\n",
    "    \n",
    "    for file in json_files:\n",
    "        if os.path.isfile(file):\n",
    "            print(f\"Loading main file: {file}\")\n",
    "            df = pd.read_json(file, orient='records', lines=True)\n",
    "            combined_main_df = pd.concat([combined_main_df, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "    \n",
    "    for file in json_files:\n",
    "        scraped_file = file.replace('.json', '_scraped.json')\n",
    "        if os.path.isfile(scraped_file):\n",
    "            print(f\"Loading scraped file: {scraped_file}\")\n",
    "            df = pd.read_json(scraped_file, orient='records', lines=True)\n",
    "            combined_scraped_df = pd.concat([combined_scraped_df, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Scraped file not found: {scraped_file}\")\n",
    "    \n",
    "    for df in [combined_main_df, combined_scraped_df]:\n",
    "        for column in df.columns:\n",
    "            if df[column].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "                df[column] = df[column].apply(lambda x: str(x) if isinstance(x, (dict, list)) else x)\n",
    "    \n",
    "    combined_main_df.drop_duplicates(inplace=True)\n",
    "    combined_scraped_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    combined_main_df.to_json(output_main_file, orient='records', lines=True)\n",
    "    combined_scraped_df.to_json(output_scraped_file, orient='records', lines=True)\n",
    "    \n",
    "    print(f\"Combined main JSON saved to: {output_main_file}\")\n",
    "    print(f\"Combined scraped JSON saved to: {output_scraped_file}\")\n",
    "\n",
    "json_files_to_combine = [\n",
    "    'List_of_culinary_fruits.json',\n",
    "    # 'List_of_birds_of_Denmark.json',\n",
    "    # 'List_of_mammals_of_Denmark.json',\n",
    "    'List_of_vegetables.json',\n",
    "    'List_of_leaf_vegetables.json',\n",
    "    'List_of_culinary_nuts.json',\n",
    "    'Plants_used_as_herbs_or_spices.json',\n",
    "    'List_of_edible_seeds.json',\n",
    "    'List_of_edible_flowers.json',\n",
    "    'List_of_forageable_plants.json'\n",
    "\n",
    "]\n",
    "\n",
    "output_main_filename = 'edible_plants.json'\n",
    "output_scraped_filename = 'edible_plants_scraped.json'\n",
    "\n",
    "combine_jsons(json_files_to_combine, output_main_filename, output_scraped_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
